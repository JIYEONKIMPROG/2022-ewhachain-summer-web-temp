{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZXn1Salxl_w"
      },
      "source": [
        "# Gemini API: All about tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FIB-JDtxgUE"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Counting_Tokens.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRzxdrjKLTJa"
      },
      "source": [
        "An understanding of tokens is central to using the Gemini API. This guide will provide a interactive introduction to what tokens are and how they are used in the Gemini API.\n",
        "\n",
        "## About tokens\n",
        "\n",
        "LLMs break up their input and produce their output at a granularity that is smaller than a word, but larger than a single character or code-point.\n",
        "\n",
        "These **tokens** can be single characters, like `z`, or whole words, like `the`. Long words may be broken up into several tokens. The set of all tokens used by the model is called the vocabulary, and the process of breaking down text into tokens is called tokenization.\n",
        "\n",
        "For Gemini models, a token is equivalent to about 4 characters. **100 tokens are about 60-80 English words**.\n",
        "\n",
        "When billing is enabled, the price of a paid request is controlled by the [number of input and output tokens](https://ai.google.dev/pricing), so knowing how to count your tokens is important.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJ1lyGC_Ia4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5027929de8f"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "Install the SDK from [PyPI](https://github.com/googleapis/python-genai)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "46zEFO2a9FFd",
        "outputId": "12caed34-b1b1-4aa1-cdc0-eaf23a358bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m713.3/713.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q -U \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTIfnvCn9HvH"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A1pkoyZb9Jm3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hx_Gw9i0Yuv"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK you now only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HghvVpbU0Uap"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDWYaeRcLBvT"
      },
      "source": [
        "## Tokens in the Gemini API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etlFvMXP3Gb7"
      },
      "source": [
        "### Context windows\n",
        "\n",
        "The models available through the Gemini API have context windows that are measured in tokens. These define how much input you can provide, and how much output the model can generate, and combined are referred to as the \"context window\". This information is available directly through [the API](https://ai.google.dev/api/rest/v1/models/get) and in the [models](https://ai.google.dev/models/gemini) documentation.\n",
        "\n",
        "In this example you can see the `gemini-2.5-flash` model has an 1M tokens context window. If you need more, Pro models have an even bigger 2M tokens context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QC23D2z3GLV"
      },
      "outputs": [],
      "source": [
        "model_info = client.models.get(model='gemini-2.5-flash')\n",
        "\n",
        "print(\"Context window:\",model_info.input_token_limit, \"tokens\")\n",
        "print(\"Max output window:\",model_info.output_token_limit, \"tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkh8v5QI4v5h"
      },
      "source": [
        "## Counting tokens\n",
        "\n",
        "The API provides an endpoint for counting the number of tokens in a request: [`client.models.count_tokens`](https://googleapis.github.io/python-genai/#count-tokens-and-compute-tokens). You pass the same arguments as you would to [`client.models.generate_content`](https://googleapis.github.io/python-genai/#generate-content) and the service will return the number of tokens in that request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvA_mbi1JxD5"
      },
      "source": [
        "### Choose a model\n",
        "\n",
        "Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off).\n",
        "\n",
        "The tokenization should be more or less the same for each of the Gemini models, but you can still switch between the different ones to double-check.\n",
        "\n",
        "For more information about all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for extended information on each of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AChpZWIXu62m"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-3-flash-preview\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-3-flash-preview\", \"gemini-3-pro-preview\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0J8JPYbCGnv"
      },
      "source": [
        "### Text tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTeA-iFjMERd"
      },
      "outputs": [],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "print(\"Prompt tokens:\",response.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0661517a2417"
      },
      "source": [
        "When you call `client.models.generate_content` (or `chat.send_message`) the response object has a `usage_metadata` attribute containing both the input, output, and thinking token counts (`prompt_token_count`, `candidates_token_count` and `thoughts_token_count`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71aa6568a670"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"The quick brown fox jumps over the lazy dog.\"\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dacccfcbf5f"
      },
      "outputs": [],
      "source": [
        "print(\"Prompt tokens:\\t \",response.usage_metadata.prompt_token_count)\n",
        "print(\"Thinking tokens:\",response.usage_metadata.thoughts_token_count)\n",
        "print(\"Output tokens:\\t \",response.usage_metadata.candidates_token_count)\n",
        "print(\"--------------\")\n",
        "print(\"Total tokens:\\t\",response.usage_metadata.total_token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoVUjJx5mdqh"
      },
      "source": [
        "In case you are using [caching](./Caching.ipynb#scrollTo=t_PWabuayrf-), the number of cached token will be indicated in `response.usage_metadata.cached_content_token_count`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZYcaUXl-Sna"
      },
      "source": [
        "### Multi-modal tokens\n",
        "\n",
        "All input to the API is tokenized, including images or other non-text modalities.\n",
        "\n",
        "Images are considered to be a fixed size, so they consume a fixed number of tokens, regardless of their display or file size.\n",
        "\n",
        "Video and audio files are converted to tokens at a fixed per second rate.\n",
        "\n",
        "The current rates and token sizes can be found on the [documentation](https://ai.google.dev/gemini-api/docs/tokens?lang=python#multimodal-tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsKfX8LYAdLv"
      },
      "outputs": [],
      "source": [
        "!curl -L https://goo.gle/instrument-img -o organ.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzwrahub-ez5"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "from IPython.display import display, Image\n",
        "\n",
        "organ = PIL.Image.open('organ.jpg')\n",
        "display(Image('organ.jpg', width=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4164419d70f"
      },
      "source": [
        "#### Inline content\n",
        "\n",
        "Media objects can be sent to the API inline with the request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ledzam3H__Ob"
      },
      "outputs": [],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=[organ]\n",
        ")\n",
        "\n",
        "print(\"Prompt with image tokens:\",response.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaBDkfB8X8b4"
      },
      "source": [
        "You can try with different images and should always get the same number of tokens, that is independent of their display or file size. Note that an extra token seems to be added, representing the empty prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3851a09ec17"
      },
      "source": [
        "#### Files API\n",
        "\n",
        "The model sees identical tokens if you upload parts of the prompt through the files API instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f994c2dd6e05"
      },
      "outputs": [],
      "source": [
        "organ_upload = client.files.upload(file='organ.jpg')\n",
        "\n",
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=organ_upload,\n",
        ")\n",
        "\n",
        "print(\"Prompt with image tokens:\",response.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8342199c9eb4"
      },
      "source": [
        "Audio and video are each converted to tokens at a fixed rate of tokens per minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be103816898c"
      },
      "outputs": [],
      "source": [
        "!curl -q -o sample.mp3  \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\"\n",
        "!ffprobe -v error -show_entries format=duration sample.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAaN-i37wMoV"
      },
      "source": [
        "As you can see, this audio file is 2610s long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ada734553530"
      },
      "outputs": [],
      "source": [
        "audio_sample = client.files.upload(file='sample.mp3')\n",
        "\n",
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=audio_sample\n",
        ")\n",
        "\n",
        "print(\"Prompt with audio tokens:\",response.total_tokens)\n",
        "print(\"Tokens per second: \",response.total_tokens/2610)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z3o3cwepitP"
      },
      "source": [
        "As you can see this corresponds to about 32 tokens per second of audio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YScILYYjwjav"
      },
      "source": [
        "### Chat, tools and caching\n",
        "\n",
        "Chat, tools and caching are currently not supported by the unified SDK `count_tokens` method. This notebook will be updated when that will be the case.\n",
        "\n",
        "In the meantime you can still check the token used after the call using the `usage_metadata` from the response. Check the [caching notebook](./Caching.ipynb#scrollTo=t_PWabuayrf-) for an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZNBWZLDCXa"
      },
      "source": [
        "## Further reading\n",
        "\n",
        "For more on token counting, check out the [documentation](https://ai.google.dev/gemini-api/docs/tokens?lang=python#multimodal-tokens) or the API reference:\n",
        "\n",
        "* [`countTokens`](https://ai.google.dev/api/rest/v1/models/countTokens) REST API reference,\n",
        "* [`count_tokens`](https://googleapis.github.io/python-genai/#count-tokens-and-compute-tokens) Python API reference,"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQKCbGWR-9aG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Counting_Tokens.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}